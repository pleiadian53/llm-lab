{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT Trainer Demo\n",
    "\n",
    "This notebook demonstrates how to use the `sft_trainer` package for supervised fine-tuning with various PEFT methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for local development\n",
    "import sys\n",
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sft_trainer import (\n",
    "    SFTTrainerWrapper,\n",
    "    TrainingConfig,\n",
    "    load_model_and_tokenizer,\n",
    "    generate_response,\n",
    "    test_model,\n",
    "    display_dataset,\n",
    ")\n",
    "from sft_trainer.peft import PEFTConfig, PEFTMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True if you have a GPU\n",
    "USE_GPU = False\n",
    "\n",
    "# Model and dataset\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "DATASET_NAME = \"banghua/DL-SFT-Dataset\"\n",
    "\n",
    "# Limit samples for quick testing\n",
    "MAX_SAMPLES = 100 if not USE_GPU else None\n",
    "\n",
    "# Test questions\n",
    "QUESTIONS = [\n",
    "    \"Give me a 1-sentence introduction of LLM.\",\n",
    "    \"Calculate 1+1-1\",\n",
    "    \"What's the difference between thread and process?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Full Fine-Tuning (No PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training configuration\n",
    "training_config = TrainingConfig(\n",
    "    learning_rate=8e-5,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=2,\n",
    "    output_dir=\"./output_full_ft\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainerWrapper(\n",
    "    model_name=MODEL_NAME,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    training_config=training_config,\n",
    "    use_gpu=USE_GPU,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "\n",
    "# Train\n",
    "metrics = trainer.train()\n",
    "print(f\"Training metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "if not USE_GPU:\n",
    "    trainer.model.to(\"cpu\")\n",
    "test_model(trainer.model, trainer.tokenizer, QUESTIONS, title=\"After Full Fine-Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LoRA preset\n",
    "peft_config = PEFTConfig.from_preset(\"lora_default\")\n",
    "print(f\"PEFT Method: {peft_config.method}\")\n",
    "print(f\"Rank: {peft_config.r}\")\n",
    "print(f\"Alpha: {peft_config.lora_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer with LoRA\n",
    "training_config_lora = TrainingConfig(\n",
    "    learning_rate=2e-4,  # Higher LR for LoRA\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=2,\n",
    "    output_dir=\"./output_lora\",\n",
    ")\n",
    "\n",
    "trainer_lora = SFTTrainerWrapper(\n",
    "    model_name=MODEL_NAME,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    training_config=training_config_lora,\n",
    "    peft_config=peft_config,\n",
    "    use_gpu=USE_GPU,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "\n",
    "# Train\n",
    "metrics_lora = trainer_lora.train()\n",
    "print(f\"Training metrics: {metrics_lora}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the LoRA model\n",
    "if not USE_GPU:\n",
    "    trainer_lora.model.to(\"cpu\")\n",
    "test_model(trainer_lora.model, trainer_lora.tokenizer, QUESTIONS, title=\"After LoRA Fine-Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom PEFT Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom LoRA config\n",
    "custom_peft = PEFTConfig(\n",
    "    method=PEFTMethod.LORA,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Only attention projections\n",
    ")\n",
    "\n",
    "print(f\"Custom config: r={custom_peft.r}, alpha={custom_peft.lora_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Available PEFT Presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available presets\n",
    "presets = [\n",
    "    \"lora_default\",\n",
    "    \"lora_high_rank\",\n",
    "    \"dora\",\n",
    "    \"olora\",\n",
    "    \"qlora_4bit\",\n",
    "    \"qlora_8bit\",\n",
    "    \"vera\",\n",
    "    \"adalora\",\n",
    "    \"ia3\",\n",
    "    \"prompt_tuning\",\n",
    "    \"prefix_tuning\",\n",
    "]\n",
    "\n",
    "print(\"Available PEFT presets:\")\n",
    "for preset in presets:\n",
    "    config = PEFTConfig.from_preset(preset)\n",
    "    print(f\"  - {preset}: {config.method.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference with Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a single response\n",
    "response = generate_response(\n",
    "    trainer_lora.model,\n",
    "    trainer_lora.tokenizer,\n",
    "    user_message=\"Explain what a neural network is in simple terms.\",\n",
    "    max_new_tokens=150,\n",
    ")\n",
    "print(\"Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "trainer_lora.save_model(\"./my_lora_adapter\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model, tokenizer = load_model_and_tokenizer(MODEL_NAME, use_gpu=USE_GPU)\n",
    "\n",
    "# Load LoRA adapter\n",
    "model_with_lora = PeftModel.from_pretrained(base_model, \"./my_lora_adapter\")\n",
    "print(\"Model loaded with LoRA adapter!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Full Fine-Tuning**: Training all model parameters\n",
    "2. **LoRA Fine-Tuning**: Using preset configurations\n",
    "3. **Custom PEFT**: Creating custom configurations\n",
    "4. **Available Presets**: All supported PEFT methods\n",
    "5. **Inference**: Generating responses\n",
    "6. **Model Persistence**: Saving and loading adapters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
