Below is a **unified, technical but intuitive** explanation of **how memory is represented, stored, updated, and propagated** across the full historical arc:

**RNN ‚Üí LSTM ‚Üí Transformer ‚Üí Efficient Transformer ‚Üí SSM (S4) ‚Üí Selective SSM (Mamba) ‚Üí RetNet ‚Üí RWKV ‚Üí Hyena ‚Üí Hybrid long-context LLMs (Jamba, StripedHyena-2, etc.)**

I‚Äôll show how each model *stores* information, *updates* it, and *retrieves* it ‚Äî because memory defines an architecture‚Äôs strengths and limits.

---

# üß≠ 0. The Core Question:

**How do sequence models store and propagate memory over long contexts?**

Every architecture answers this differently:

* **RNN:** One compact moving hidden vector
* **LSTM/GRU:** Gated moving hidden vector
* **Transformer:** Externalized memory in all tokens (attention)
* **Efficient Transformers:** Sparse / compressed memory of tokens
* **SSMs (S4):** Memory encoded in long convolution kernels
* **Mamba:** Input-dependent recurrent memory
* **RetNet:** Exponentially weighted retention states
* **RWKV:** Transformer-like softmax memory inside RNN recurrence
* **Hyena:** Convolutional implicit memory across long horizons
* **Hybrid LLMs:** Memory is distributed across attention + SSM + conv + caches + retrieval layers

Let‚Äôs walk through the evolution.

---

# 1Ô∏è‚É£ Early RNNs: Memory as a Single Hidden Vector

### Memory Representation

A single hidden vector ( h_t \in \mathbb{R}^d ).

### Update Rule

[
h_t = \tanh(W_h h_{t-1} + W_x x_t)
]

### How Memory Propagates

* The entire past is compressed into (h_t).
* Memory decays exponentially due to repeated multiplication by (W_h).
* Vanishing gradients prevent learning long-range dependencies.

### Strengths

* Streaming
* Constant memory use
* Simple recurrence

### Weakness

* **Catastrophic forgetting**
* Long-range dependencies nearly impossible

---

# 2Ô∏è‚É£ LSTMs / GRUs: Gated Recurrent Memory

### Memory Representation

Two vectors:

* cell state (c_t) = long-term memory
* hidden state (h_t) = output memory

### Update Rules (abbreviated)

[
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
]

[
h_t = o_t \odot \tanh(c_t)
]

### How Memory Propagates

* Forget gate (f_t) controls how much memory is kept
* Additive path in (c_t) mitigates vanishing gradients
* Still maintains **one compressed vector** representing all past

### Strengths

* Better long-range modeling
* Still fully streaming

### Limitations

* Still compresses the entire past into a single vector
* Cannot represent many independent memories

---

# 3Ô∏è‚É£ Transformers (2017):

### ‚ÄúMemory = All Tokens, Stored Explicitly.‚Äù

This was the revolution.

### Memory Representation

For a sequence length (N), memory = the set of all hidden states:

[
M = [x_1, x_2, \ldots, x_N]
]

### Update via Attention

A token at position (t) accesses **all previous hidden states**:

[
\text{Attention}(t) = \sum_{i \le t} \alpha_{t,i} , V_i
]

where (\alpha_{t,i}) are softmax weights from similarity of (Q_t) and (K_i).

### How Memory Propagates

* No compression: **every token keeps its own memory**
* The model retrieves memory via similarity search
* Perfect for complex dependencies

### Strengths

* No vanishing gradient
* High-capacity memory
* Great for reasoning and pattern matching

### Weakness

* Memory grows **O(N¬≤)** in compute and **O(N)** in storage
* Cannot stream naturally

---

# 4Ô∏è‚É£ Efficient Transformers:

### ‚ÄúAttention, but with Cheaper or Compressed Memory.‚Äù

Different strategies change how memory is stored:

---

## 4.1 Sparse Attention (Longformer, BigBird)

### Memory

Only selected tokens are remembered (local windows, global tokens).

### Propagation

[
\alpha_{t,i} = 0 \quad \text{for most } i
]

### Effect

Cheaper, but loses global memory connections.

---

## 4.2 Low-Rank Attention (Linformer, Performer)

### Memory Representation

Keys and values projected to smaller dimension:

[
K' = E_K K,\quad V' = E_V V
]

### Effect

Memory is **compressed** linearly.

---

## 4.3 Memory Caching (Transformer-XL, Compressive Transformer)

Caches previous segments and compress older memories into ‚Äúsummaries.‚Äù

### Memory Propagation

[
M = M_{\text{cached}} \cup M_{\text{current}}
]

### Strength

Long-range context (1M+ tokens) but with degradations.

---

# 5Ô∏è‚É£ S4 (Structured State Space Models):

### ‚ÄúMemory = A Very Long Convolution Kernel Learned from a Dynamical System.‚Äù

### Memory Representation

A dynamical hidden state:

[
h_{t+1} = \bar{A} h_t + \bar{B} x_t
]

Output:

[
y_t = C h_t
]

### Convolutional View

Unroll the recurrence:

[
y_t = \sum_{\tau=0}^{t} K_{\tau} x_{t-\tau}
]

Memory is encoded inside (K_\tau = C \bar{A}^\tau \bar{B}).

### How Memory Propagates

* Via **convolution kernel** across time
* Kernel may be extremely long (>100k positions)
* Memory decay controlled by eigenvalues of (\bar{A})
* HiPPO initialization ensures long-range retention

### Strengths

* **Linear-time**
* Strong long-range capabilities
* No need to store tokens

### Weakness

* Lacks content-based recall (unlike attention)

---

# 6Ô∏è‚É£ Mamba (Selective SSM):

### ‚ÄúMemory = Input-dependent state transitions.‚Äù

### Memory Representation

State:

[
h_t
]

### Update

[
h_t = A(x_t) h_{t-1} + B(x_t) x_t
]

* Transition matrix changes with the token ‚Üí **adaptive memory**
* Memory becomes both structured **and content-based**

### Propagation

* Like S4‚Äôs convolution, but now adaptive
* Memory can selectively retain or erase based on content
* Great for selective recall

### Strengths

* Linear-time
* Streaming-native
* Near-attention performance

---

# 7Ô∏è‚É£ RetNet (Retentive Networks)

### ‚ÄúMemory = Exponentially decayed running sum of key/value interactions.‚Äù

### Memory Representation

Retention state:

[
s_t = \gamma \odot s_{t-1} + K_t^\top V_t
]

Output:

[
y_t = Q_t s_t
]

### How Memory Propagates

* Exponential decay controls how long information survives
* Can run recurrently (streaming) or in parallel (like attention)
* Acts like ‚Äúsoft attention with forgetting‚Äù

### Strengths

* Linear-time
* Good long-range retention
* Attention-like but cheaper

---

# 8Ô∏è‚É£ RWKV

### ‚ÄúMemory = Attention-shaped gates inside an RNN.‚Äù

RWKV blends RNN recurrence with attention math.

### Memory Representation

State:

[
w_t, ; k_t, ; v_t
]

### Update (simplified)

[
w_t = \alpha_t \cdot w_{t-1} + \beta_t
]

[
\text{output}_t = \frac{\sum w_i , v_i}{\sum w_i}
]

This looks like attention, but updated **recurrently**.

### How Memory Propagates

* Through exponential moving averages
* Selective gating mimics attention weight rereading
* Parallelizable in training but streaming in inference

### Strengths

* Long memory
* Low compute
* Fully streaming

---

# 9Ô∏è‚É£ Hyena / StripedHyena

### ‚ÄúMemory = Long-range implicit convolutions + gated mixing.‚Äù

### Memory Representation

Convolution kernel (K(\tau)) generated from a small function:

[
y_t = \sum_{\tau} K(\tau) \odot x_{t-\tau}
]

### How Memory Propagates

* Flexible kernels represent very long-range dependencies
* Convolutions computed via FFT ‚Üí **linear or near-linear**
* Gating adds nonlinearity like attention

### Strengths

* Extreme long-range memory (1M+ tokens)
* Efficient
* Strong on DNA/protein sequences

---

# üîü Modern Hybrid LLMs (2024‚Äì2025)

(Jamba, StripedHyena-2, Qwen-Long, GPT-5, Claude 3.7 long-context)

### ‚ÄúMemory = a mixture of attention + SSM + convolution + caches.‚Äù

### Memory Sources

1. **Attention** for precise local recall
2. **SSM/Mamba** for cheap long memory
3. **Hyena/conv** for frequency-based long patterns
4. **Retrieval layers** (RAG) for external memory
5. **KV caching** for sliding-window attention
6. **Long-range recurrent states**
7. **MoE experts** as specialized memory modules
8. **Hypernetwork conditioning** for meta-memory
9. **Reasoning PRMs / chain-of-thought** for process memory

### Memory Propagation

Modern LLMs distribute memory across:

* **Parallelizable attention layers**
* **Streaming SSM layers**
* **Convolutional layers**
* **Optional retrieval context**
* **Cached KV history**

This hybrid soup enables:

* **1M‚Äì10M+ context windows**
* Near-linear scaling
* Rich reasoning
* Streaming and parallelization
* Factual consistency via retrieval

This is the ‚Äúpost-Transformer‚Äù era.

---

# üß© Final Summary Table: Evolution of Memory

| Model Type                 | Memory Representation              | How Memory Propagates   | Strengths             | Weaknesses              |
| -------------------------- | ---------------------------------- | ----------------------- | --------------------- | ----------------------- |
| **RNN**                    | (h_t)                              | multiply + tanh         | simple, streaming     | forgets                 |
| **LSTM/GRU**               | (c_t, h_t)                         | gated additive memory   | better retention      | still compressed        |
| **Transformer**            | all token states                   | attention over tokens   | rich memory           | O(N¬≤)                   |
| **Efficient Transformers** | sparse or low-rank token memory    | approximated attention  | longer context        | less precise            |
| **S4 (SSM)**               | convolution kernel                 | linear dynamical system | very long context     | no content lookup       |
| **Mamba**                  | selective SSM state                | adaptive recurrence     | attention-like recall | complex training        |
| **RetNet**                 | retention states                   | exponential mixing      | hybrid attention      | less precision          |
| **RWKV**                   | EMA-like weighted state            | recurrent attention     | streaming             | less global interaction |
| **Hyena**                  | implicit long conv                 | FFT-based propagation   | extreme long memory   | limited content recall  |
| **Hybrid LLMs**            | attention + SSM + conv + retrieval | multi-path memory       | best SoA              | complex architecture    |

---

If you want, I can now continue with one of the following:

### ‚úîÔ∏è A unified mathematical framework showing how all these models are ‚Äúgeneralized recurrences‚Äù

### ‚úîÔ∏è Compare their frequency-domain behavior (SSM vs attention vs conv)

### ‚úîÔ∏è Build visual diagrams showing memory flow

### ‚úîÔ∏è Write this as a LaTeX-ready chapter for your **BioGraphLab Notes**

### ‚úîÔ∏è Give you PyTorch mini-implementations of each memory mechanism

Just tell me which direction you want next.
